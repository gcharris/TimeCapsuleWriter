#!/usr/bin/env python3
"""
Model Comparison Tool for TimeCapsuleWriter

Generates the same story using different models and saves outputs in markdown format
for easy comparison.

Usage:
    # Compare default models with a simple prompt
    python model_comparison.py --prompt "A mystery in foggy London"
    
    # Compare specific models
    python model_comparison.py --models haykgrigo3/TimeCapsuleLLM microsoft/phi-3-mini-4k-instruct
    
    # Use a specific seed for reproducibility
    python model_comparison.py --seed 42 --prompt "A ghost story set in Yorkshire"
    
    # Use a character profile and save to a specific directory
    python model_comparison.py --characters clerk --output_dir comparisons/test1
"""

import os
import sys
import argparse
import torch
import time
import yaml
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

# Import TimeCapsuleWriter modules
import config
from main import setup_generator
from character_utils import integrate_characters_with_prompt, load_text_file
from bench_models import measure_memory, analyze_output_quality

# Default models to compare
DEFAULT_MODELS = [
    "haykgrigo3/TimeCapsuleLLM",        # Main model
    "haykgrigo3/TimeCapsuleLLM-medium", # Medium version (if it exists)
    "haykgrigo3/TimeCapsuleLLM-small",  # Small version (if it exists)
]

# Fallback if the specialized models don't exist
FALLBACK_MODELS = [
    "haykgrigo3/TimeCapsuleLLM",       # Main model
    "microsoft/phi-3-mini-4k-instruct", # Lightweight alternative
    "facebook/opt-125m",               # Tiny model for testing
]

def format_story_markdown(
    title: str, 
    model_id: str, 
    prompt: str, 
    output_text: str, 
    metrics: Optional[Dict[str, Any]] = None
) -> str:
    """
    Format a story as a markdown document with metadata.
    
    Args:
        title: Story title
        model_id: Model ID that generated the story
        prompt: The prompt used
        output_text: The generated story text
        metrics: Optional performance metrics
        
    Returns:
        Formatted markdown string
    """
    # Clean up model name for display
    model_name = model_id.split("/")[-1] if "/" in model_id else model_id
    
    # Create markdown content
    markdown = f"# {title}\n\n"
    markdown += f"*Generated by {model_name} on {datetime.now().strftime('%Y-%m-%d at %H:%M')}*\n\n"
    
    # Add metrics if provided
    if metrics:
        markdown += "## Generation Metrics\n\n"
        markdown += "| Metric | Value |\n"
        markdown += "|--------|-------|\n"
        for key, value in metrics.items():
            if isinstance(value, float):
                markdown += f"| {key} | {value:.2f} |\n"
            else:
                markdown += f"| {key} | {value} |\n"
        markdown += "\n"
    
    # Add prompt section
    markdown += "## Prompt\n\n"
    markdown += f"```\n{prompt}\n```\n\n"
    
    # Add the story content
    markdown += "## Story\n\n"
    
    # Format the story text with proper markdown
    story_text = output_text
    # Add paragraph breaks for readability
    story_text = story_text.replace("\n\n", "\n\n\n")
    
    markdown += story_text
    
    return markdown

def generate_with_model(
    model_id: str, 
    persona: str,
    prompt_text: str, 
    seed: Optional[int] = None, 
    characters: Optional[List[str]] = None,
    max_new_tokens: int = 450
) -> Dict[str, Any]:
    """
    Generate a story using the specified model and collect performance metrics.
    
    Args:
        model_id: HuggingFace model ID
        persona: Persona prompt text
        prompt_text: Story prompt text
        seed: Optional random seed
        characters: Optional list of character names to include
        max_new_tokens: Maximum number of tokens to generate
        
    Returns:
        Dictionary with story text and metrics
    """
    print(f"\nGenerating with model: {model_id}")
    
    # Set up arguments namespace
    class Args:
        def __init__(self):
            self.model = model_id
            self.seed = seed
            self.max_new_tokens = max_new_tokens
            self.temperature = 0.9
            self.top_p = 0.95
            self.repetition_penalty = 1.1
            self.characters = characters
    
    args = Args()
    
    # Measure performance
    start_memory = measure_memory()
    start_time = time.time()
    
    # Set up the generator
    try:
        generator = setup_generator(args)
        
        # Construct the full prompt
        full_prompt = f"{persona}\n\nWrite a Victorian-era short story based on this concept:\n\n{prompt_text}"
        
        # Add character profiles if specified
        if characters:
            print(f"  Including characters: {', '.join(characters)}")
            full_prompt = integrate_characters_with_prompt(full_prompt, characters)
        
        # Generate text
        print("  Generating story...")
        response = generator(full_prompt, return_full_text=False)[0]['generated_text']
        
        # Calculate metrics
        gen_time = time.time() - start_time
        memory_used = measure_memory() - start_memory
        
        # Analyze quality
        quality_metrics = analyze_output_quality(response)
        
        # Combine all metrics
        metrics = {
            "generation_time_seconds": gen_time,
            "memory_used_mb": memory_used,
            "tokens_per_second": len(response.split()) / gen_time,
            **quality_metrics
        }
        
        return {
            "text": response,
            "metrics": metrics,
            "success": True
        }
    
    except Exception as e:
        print(f"  Error: {str(e)}")
        return {
            "text": f"Error generating with model {model_id}: {str(e)}",
            "metrics": {
                "generation_time_seconds": time.time() - start_time,
                "memory_used_mb": measure_memory() - start_memory,
                "error": str(e)
            },
            "success": False
        }

def save_comparison_results(
    results: Dict[str, Dict[str, Any]], 
    title: str, 
    prompt: str, 
    output_dir: str
) -> List[str]:
    """
    Save the comparison results as markdown files.
    
    Args:
        results: Dictionary mapping model IDs to generation results
        title: Story title
        prompt: Story prompt text
        output_dir: Directory to save results
        
    Returns:
        List of saved file paths
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Create a timestamp for the files
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Save individual model results
    saved_files = []
    
    for model_id, result in results.items():
        # Clean model name for filename
        model_name = model_id.split("/")[-1] if "/" in model_id else model_id
        model_name = model_name.replace("-", "_").lower()
        
        # Create markdown content
        markdown = format_story_markdown(
            title=title,
            model_id=model_id,
            prompt=prompt,
            output_text=result["text"],
            metrics=result["metrics"]
        )
        
        # Save to file
        filename = f"{timestamp}_{model_name}.md"
        file_path = os.path.join(output_dir, filename)
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(markdown)
        
        saved_files.append(file_path)
        print(f"Saved result for {model_id} to {file_path}")
    
    # Create a comparison summary
    comparison_path = os.path.join(output_dir, f"{timestamp}_comparison.md")
    with open(comparison_path, "w", encoding="utf-8") as f:
        f.write(f"# Model Comparison: {title}\n\n")
        f.write(f"*Generated on {datetime.now().strftime('%Y-%m-%d at %H:%M')}*\n\n")
        
        # Add prompt section
        f.write("## Prompt\n\n")
        f.write(f"```\n{prompt}\n```\n\n")
        
        # Add metrics comparison table
        f.write("## Performance Comparison\n\n")
        f.write("| Model | Gen Time (s) | Memory (MB) | Tokens/s | Victorian Terms | Word Count |\n")
        f.write("|-------|--------------|-------------|----------|-----------------|------------|\n")
        
        for model_id, result in results.items():
            model_name = model_id.split("/")[-1]
            metrics = result["metrics"]
            
            # Only include successful generations in the table
            if result["success"]:
                gen_time = f"{metrics.get('generation_time_seconds', 'N/A'):.2f}"
                memory = f"{metrics.get('memory_used_mb', 'N/A'):.1f}"
                tokens_per_sec = f"{metrics.get('tokens_per_second', 'N/A'):.1f}"
                vic_terms = metrics.get("victorian_term_count", "N/A")
                word_count = metrics.get("word_count", "N/A")
                
                f.write(f"| {model_name} | {gen_time} | {memory} | {tokens_per_sec} | {vic_terms} | {word_count} |\n")
            else:
                f.write(f"| {model_name} | Failed | N/A | N/A | N/A | N/A |\n")
        
        # Add links to individual files
        f.write("\n## Individual Results\n\n")
        for model_id, file_path in zip(results.keys(), saved_files):
            model_name = model_id.split("/")[-1]
            rel_path = os.path.basename(file_path)
            f.write(f"- [{model_name}]({rel_path})\n")
    
    saved_files.append(comparison_path)
    print(f"Saved comparison summary to {comparison_path}")
    
    return saved_files

def main():
    """Main entry point for the script."""
    parser = argparse.ArgumentParser(description="Generate and compare stories using different models")
    
    # Model selection
    parser.add_argument("--models", nargs="+", help="Models to compare")
    
    # Content parameters
    parser.add_argument("--prompt", default="A mystery in foggy London", 
                        help="Story concept or prompt")
    parser.add_argument("--title", help="Story title (defaults to prompt if not provided)")
    parser.add_argument("--persona", 
                        default=os.path.join(config.PROMPTS_DIR, "persona_victorian.md"),
                        help="Path to persona prompt file")
    parser.add_argument("--characters", nargs="+", help="Character profiles to include")
    
    # Generation parameters
    parser.add_argument("--seed", type=int, help="Random seed for reproducibility")
    parser.add_argument("--max_new_tokens", type=int, default=450,
                        help="Maximum number of tokens to generate")
    
    # Output options
    parser.add_argument("--output_dir", default="comparisons", 
                        help="Directory to save comparison results")
    
    args = parser.parse_args()
    
    # Determine which models to use
    models_to_use = args.models if args.models else DEFAULT_MODELS
    
    # Load persona
    persona_text = load_text_file(args.persona)
    
    # Determine story title
    title = args.title if args.title else args.prompt
    
    # Generate with each model
    results = {}
    
    for model_id in models_to_use:
        result = generate_with_model(
            model_id=model_id,
            persona=persona_text,
            prompt_text=args.prompt,
            seed=args.seed,
            characters=args.characters,
            max_new_tokens=args.max_new_tokens
        )
        results[model_id] = result
    
    # Save the results
    save_comparison_results(
        results=results,
        title=title,
        prompt=args.prompt,
        output_dir=args.output_dir
    )
    
    print("\nComparison complete!")

if __name__ == "__main__":
    main()
